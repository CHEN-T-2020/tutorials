{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "print(\"当前目录是：\", os.getcwd())\n",
    "# 验证是否正确加载\n",
    "print(\"Langsmith API Key:\", os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "print(\"Langsmith Project:\", os.getenv(\"LANGSMITH_PROJECT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI()\n",
    "response = llm.invoke(\"Hello, world!\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 LangSmith 客户端\n",
    "client = Client()\n",
    "\n",
    "# 尝试列出数据集，验证 API 是否正常工作\n",
    "try:\n",
    "    datasets = client.list_datasets()\n",
    "    print(\"✅ 成功连接到 LangSmith API！可用数据集数量：\", len(list(datasets)))\n",
    "except Exception as e:\n",
    "    print(\"❌ 无法连接到 LangSmith API\")\n",
    "    print(\"错误信息:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a Dataset (Only Inputs, No Output)\n",
    "\n",
    "example_inputs = [\n",
    "    \"a rap battle between Atticus Finch and Cicero\",\n",
    "    \"a rap battle between Barbie and Oppenheimer\",\n",
    "    \"a Pythonic rap battle between two swallows: one European and one African\",\n",
    "    \"a rap battle between Aubrey Plaza and Stephen Colbert\",\n",
    "]\n",
    "\n",
    "dataset_name = \"Rap Battle Dataset\"\n",
    "\n",
    "# Storing inputs in a dataset lets us\n",
    "# run chains and LLMs over a shared set of examples.\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Rap battle prompts.\",\n",
    ")\n",
    "\n",
    "for question in example_inputs:\n",
    "    # Each example must be unique and have inputs defined.\n",
    "    # Outputs are optional\n",
    "    client.create_example(\n",
    "        inputs={\"question\": question},\n",
    "        outputs=None,\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Evaluate Datasets with LLM\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can specify an evaluator by name/enum.\n",
    "        # In this case, the default criterion is \"helpfulness\"\n",
    "        \"criteria\",\n",
    "        # Or you can configure the evaluator\n",
    "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
    "        RunEvalConfig.Criteria(\"misogyny\"),\n",
    "        RunEvalConfig.Criteria(\n",
    "            {\n",
    "                \"cliche\": \"Are the lyrics cliche? \"\n",
    "                \"Respond Y if they are, N if they're entirely unique.\"\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=eval_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Ways of Creating Datasets in LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a Dataset From a List of Examples (Key-Value Pairs)\n",
    "\n",
    "example_inputs = [\n",
    "    (\"What is the largest mammal?\", \"The blue whale\"),\n",
    "    (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),\n",
    "    (\"What are reptiles known for?\", \"Having scales\"),\n",
    "    (\n",
    "        \"What's the main characteristic of amphibians?\",\n",
    "        \"They live both in water and on land\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "dataset_name = \"Elementary Animal Questions\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Questions and answers about animal phylogenetics.\",\n",
    ")\n",
    "\n",
    "for question, answer in example_inputs:\n",
    "    client.create_example(\n",
    "        inputs={\"question\": question},\n",
    "        outputs={\"answer\": answer},\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a Dataset From a Dataframe\n",
    "\n",
    "# Create a Dataframe\n",
    "\n",
    "example_inputs = [\n",
    "    (\"What is the largest mammal?\", \"The blue whale\"),\n",
    "    (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),\n",
    "    (\"What are reptiles known for?\", \"Having scales\"),\n",
    "    (\n",
    "        \"What's the main characteristic of amphibians?\",\n",
    "        \"They live both in water and on land\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "df_dataset = pd.DataFrame(example_inputs, columns=[\"Question\", \"Answer\"])\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_keys = [\"Question\"]\n",
    "output_keys = [\"Answer\"]\n",
    "\n",
    "# Create Dataset\n",
    "\n",
    "dataset = client.upload_dataframe(\n",
    "    df=df_dataset,\n",
    "    input_keys=input_keys,\n",
    "    output_keys=output_keys,\n",
    "    name=\"My Dataframe Dataset\",\n",
    "    description=\"Dataset created from a dataframe\",\n",
    "    data_type=\"kv\",  # The default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a Dataset From a CSV File\n",
    "\n",
    "# Save the Dataframe as a CSV File\n",
    "\n",
    "csv_path = \"./data/dataset.csv\"\n",
    "df_dataset.to_csv(csv_path, index=False)\n",
    "\n",
    "# Create Dataset\n",
    "\n",
    "dataset = client.upload_csv(\n",
    "    csv_file=csv_path,\n",
    "    input_keys=input_keys,\n",
    "    output_keys=output_keys,\n",
    "    name=\"My CSV Dataset\",\n",
    "    description=\"Dataset created from a CSV file\",\n",
    "    data_type=\"kv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correctness: LangSmith Question-Answer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Evaluate Datasets That Contain Labels\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        \"qa\",  # correctness: right or wrong\n",
    "        \"context_qa\",  # refer to example outputs\n",
    "        \"cot_qa\",  # context_qa + reasoning\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Elementary Animal Questions\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Evaluate Datasets With Customized Criterias\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\n",
    "        RunEvalConfig.LabeledCriteria(\n",
    "            {\n",
    "                \"helpfulness\": (\n",
    "                    \"Is this submission helpful to the user,\"\n",
    "                    \" taking into account the correct reference answer?\"\n",
    "                )\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Elementary Animal Questions\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Evaluate Datasets Without Labels\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\n",
    "        RunEvalConfig.Criteria(\n",
    "            {\"creativity\": \"Is this submission creative, imaginative, or novel?\"}\n",
    "        ),\n",
    "        # We provide some simple default criteria like \"conciseness\" you can use as well\n",
    "        RunEvalConfig.Criteria(\"conciseness\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Rap Battle Dataset\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate Datasets Based on Cosine Distance Criteria\n",
    "# Cosine Distance: Ranged Between 0 to 1. 0 = More Similar\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\n",
    "        \"embedding_distance\",\n",
    "        # Or to customize the embeddings:\n",
    "        # Requires 'pip install sentence_transformers'\n",
    "        # RunEvalConfig.EmbeddingDistance(embeddings=HuggingFaceEmbeddings(), distance_metric=\"cosine\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Elementary Animal Questions\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前数据集: [Dataset(name='Elementary Animal Questions', description='Questions and answers about animal phylogenetics.', data_type=<DataType.kv: 'kv'>, id=UUID('d8682549-f339-411b-9772-261bdddff364'), created_at=datetime.datetime(2025, 6, 26, 5, 36, 38, 26943, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 6, 26, 5, 36, 38, 26943, tzinfo=datetime.timezone.utc), example_count=4, session_count=3, last_session_start_time=datetime.datetime(2025, 6, 26, 5, 44, 5, 953506), inputs_schema=None, outputs_schema=None, transformations=None), Dataset(name='Rap Battle Dataset', description='Rap battle prompts.', data_type=<DataType.kv: 'kv'>, id=UUID('2013dbcd-1ca9-4e41-b1a3-9990b2b4698c'), created_at=datetime.datetime(2025, 6, 26, 5, 13, 36, 904799, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 6, 26, 5, 13, 36, 904799, tzinfo=datetime.timezone.utc), example_count=4, session_count=5, last_session_start_time=datetime.datetime(2025, 6, 26, 5, 43, 43, 101001), inputs_schema=None, outputs_schema=None, transformations=None), Dataset(name='Example Dataset', description='An example dataset', data_type=<DataType.kv: 'kv'>, id=UUID('ee4c8564-b6e7-4e40-aaea-573b21f37e57'), created_at=datetime.datetime(2025, 6, 26, 5, 38, 48, 527973, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 6, 26, 5, 38, 48, 527973, tzinfo=datetime.timezone.utc), example_count=7, session_count=0, last_session_start_time=None, inputs_schema=None, outputs_schema=None, transformations=None), Dataset(name='My CSV Dataset', description='Dataset created from a CSV file', data_type=<DataType.kv: 'kv'>, id=UUID('416fc082-2f9c-498a-bbea-6c92b7cb6c37'), created_at=datetime.datetime(2025, 6, 26, 5, 41, 26, 291458, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 6, 26, 5, 41, 26, 291458, tzinfo=datetime.timezone.utc), example_count=4, session_count=0, last_session_start_time=None, inputs_schema=None, outputs_schema=None, transformations=None), Dataset(name='My Dataframe Dataset', description='Dataset created from a dataframe', data_type=<DataType.kv: 'kv'>, id=UUID('fa68099a-c20d-4b27-aa4d-894a05e3d7f2'), created_at=datetime.datetime(2025, 6, 26, 5, 40, 30, 86623, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 6, 26, 5, 40, 30, 86623, tzinfo=datetime.timezone.utc), example_count=4, session_count=0, last_session_start_time=None, inputs_schema=None, outputs_schema=None, transformations=None)] \n",
      "🗑️ 删除数据集: Elementary Animal Questions (ID: d8682549-f339-411b-9772-261bdddff364)\n",
      "🗑️ 删除数据集: Rap Battle Dataset (ID: 2013dbcd-1ca9-4e41-b1a3-9990b2b4698c)\n",
      "🗑️ 删除数据集: Example Dataset (ID: ee4c8564-b6e7-4e40-aaea-573b21f37e57)\n",
      "🗑️ 删除数据集: My CSV Dataset (ID: 416fc082-2f9c-498a-bbea-6c92b7cb6c37)\n",
      "🗑️ 删除数据集: My Dataframe Dataset (ID: fa68099a-c20d-4b27-aa4d-894a05e3d7f2)\n"
     ]
    }
   ],
   "source": [
    "# 删除数据集\n",
    "\n",
    "# 获取所有数据集\n",
    "datasets = list(client.list_datasets())\n",
    "print(f\"当前数据集: {datasets} \")\n",
    "\n",
    "# ✅ 批量删除\n",
    "for ds in datasets:\n",
    "    print(f\"🗑️ 删除数据集: {ds.name} (ID: {ds.id})\")\n",
    "    client.delete_dataset(dataset_id = ds.id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
