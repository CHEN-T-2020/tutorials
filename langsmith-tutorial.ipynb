{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "import os\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "from langchain_openai import ChatOpenAI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂΩìÂâçÁõÆÂΩïÊòØÔºö /Users/chen/Desktop/Code/tutorials\n",
      "Langsmith API Key: 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\n",
      "Langsmith Project: test-project-langgraph\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())\n",
    "print(\"ÂΩìÂâçÁõÆÂΩïÊòØÔºö\", os.getcwd())\n",
    "# È™åËØÅÊòØÂê¶Ê≠£Á°ÆÂä†ËΩΩ\n",
    "print(\"Langsmith API Key:\", os.getenv(\"LANGSMITH_API_KEY\"))\n",
    "print(\"Langsmith Project:\", os.getenv(\"LANGSMITH_PROJECT\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "\n",
    "llm = ChatOpenAI()\n",
    "response = llm.invoke(\"Hello, world!\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÊàêÂäüËøûÊé•Âà∞ LangSmith APIÔºÅÂèØÁî®Êï∞ÊçÆÈõÜÊï∞ÈáèÔºö 1\n"
     ]
    }
   ],
   "source": [
    "# ÂàõÂª∫ LangSmith ÂÆ¢Êà∑Á´Ø\n",
    "client = Client()\n",
    "\n",
    "# Â∞ùËØïÂàóÂá∫Êï∞ÊçÆÈõÜÔºåÈ™åËØÅ API ÊòØÂê¶Ê≠£Â∏∏Â∑•‰Ωú\n",
    "try:\n",
    "    datasets = client.list_datasets()\n",
    "    print(\"‚úÖ ÊàêÂäüËøûÊé•Âà∞ LangSmith APIÔºÅÂèØÁî®Êï∞ÊçÆÈõÜÊï∞ÈáèÔºö\", len(list(datasets)))\n",
    "except Exception as e:\n",
    "    print(\"‚ùå Êó†Ê≥ïËøûÊé•Âà∞ LangSmith API\")\n",
    "    print(\"ÈîôËØØ‰ø°ÊÅØ:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a Dataset (Only Inputs, No Output)\n",
    "\n",
    "example_inputs = [\n",
    "    \"a rap battle between Atticus Finch and Cicero\",\n",
    "    \"a rap battle between Barbie and Oppenheimer\",\n",
    "    \"a Pythonic rap battle between two swallows: one European and one African\",\n",
    "    \"a rap battle between Aubrey Plaza and Stephen Colbert\",\n",
    "]\n",
    "\n",
    "dataset_name = \"Rap Battle Dataset\"\n",
    "\n",
    "# Storing inputs in a dataset lets us\n",
    "# run chains and LLMs over a shared set of examples.\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Rap battle prompts.\",\n",
    ")\n",
    "\n",
    "for question in example_inputs:\n",
    "    # Each example must be unique and have inputs defined.\n",
    "    # Outputs are optional\n",
    "    client.create_example(\n",
    "        inputs={\"question\": question},\n",
    "        outputs=None,\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Evaluate Datasets with LLM\n",
    "\n",
    "eval_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can specify an evaluator by name/enum.\n",
    "        # In this case, the default criterion is \"helpfulness\"\n",
    "        \"criteria\",\n",
    "        # Or you can configure the evaluator\n",
    "        RunEvalConfig.Criteria(\"harmfulness\"),\n",
    "        RunEvalConfig.Criteria(\"misogyny\"),\n",
    "        RunEvalConfig.Criteria(\n",
    "            {\n",
    "                \"cliche\": \"Are the lyrics cliche? \"\n",
    "                \"Respond Y if they are, N if they're entirely unique.\"\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=dataset_name,\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=eval_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Ways of Creating Datasets in LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create a Dataset From a List of Examples (Key-Value Pairs)\n",
    "\n",
    "example_inputs = [\n",
    "    (\"What is the largest mammal?\", \"The blue whale\"),\n",
    "    (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),\n",
    "    (\"What are reptiles known for?\", \"Having scales\"),\n",
    "    (\n",
    "        \"What's the main characteristic of amphibians?\",\n",
    "        \"They live both in water and on land\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "dataset_name = \"Elementary Animal Questions\"\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Questions and answers about animal phylogenetics.\",\n",
    ")\n",
    "\n",
    "for question, answer in example_inputs:\n",
    "    client.create_example(\n",
    "        inputs={\"question\": question},\n",
    "        outputs={\"answer\": answer},\n",
    "        dataset_id=dataset.id,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Create a Dataset From a Dataframe\n",
    "\n",
    "# Create a Dataframe\n",
    "\n",
    "example_inputs = [\n",
    "    (\"What is the largest mammal?\", \"The blue whale\"),\n",
    "    (\"What do mammals and birds have in common?\", \"They are both warm-blooded\"),\n",
    "    (\"What are reptiles known for?\", \"Having scales\"),\n",
    "    (\n",
    "        \"What's the main characteristic of amphibians?\",\n",
    "        \"They live both in water and on land\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "df_dataset = pd.DataFrame(example_inputs, columns=[\"Question\", \"Answer\"])\n",
    "df_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_keys = [\"Question\"]\n",
    "output_keys = [\"Answer\"]\n",
    "\n",
    "# Create Dataset\n",
    "\n",
    "dataset = client.upload_dataframe(\n",
    "    df=df_dataset,\n",
    "    input_keys=input_keys,\n",
    "    output_keys=output_keys,\n",
    "    name=\"My Dataframe Dataset\",\n",
    "    description=\"Dataset created from a dataframe\",\n",
    "    data_type=\"kv\",  # The default\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Create a Dataset From a CSV File\n",
    "\n",
    "# Save the Dataframe as a CSV File\n",
    "\n",
    "csv_path = \"./data/dataset.csv\"\n",
    "df_dataset.to_csv(csv_path, index=False)\n",
    "\n",
    "# Create Dataset\n",
    "\n",
    "dataset = client.upload_csv(\n",
    "    csv_file=csv_path,\n",
    "    input_keys=input_keys,\n",
    "    output_keys=output_keys,\n",
    "    name=\"My CSV Dataset\",\n",
    "    description=\"Dataset created from a CSV file\",\n",
    "    data_type=\"kv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correctness: LangSmith Question-Answer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Evaluate Datasets That Contain Labels\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        \"qa\",  # correctness: right or wrong\n",
    "        \"context_qa\",  # refer to example outputs\n",
    "        \"cot_qa\",  # context_qa + reasoning\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Elementary Animal Questions\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View the evaluation results for project 'advanced-mom-49' at:\n",
      "https://smith.langchain.com/o/71f16ca1-1af3-4c25-8798-55db383a73f5/datasets/1f318077-f55f-4498-87f9-f7770a31bc01/compare?selectedSessions=acd2e84b-bfcd-46db-b37b-243b431908ad\n",
      "\n",
      "View all tests for Dataset Elementary Animal Questions at:\n",
      "https://smith.langchain.com/o/71f16ca1-1af3-4c25-8798-55db383a73f5/datasets/1f318077-f55f-4498-87f9-f7770a31bc01\n",
      "[------------------------------------------------->] 4/4"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'project_name': 'advanced-mom-49',\n",
       " 'results': {'038ad58c-7f11-43ed-b8b0-6d82033e5e90': {'input': {'question': \"What's the main characteristic of amphibians?\"},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"The criterion for this task is the helpfulness of the submission, taking into account the correct reference answer. \\n\\nThe reference answer states that the main characteristic of amphibians is that they live both in water and on land. \\n\\nThe AI's submission includes this information, stating that amphibians have a dual life cycle and typically live both on land and in water. \\n\\nIn addition to this, the AI's submission provides further information about amphibians, such as the fact that they are cold-blooded, have moist, smooth skin, begin life as water-breathing larvae, metamorphose into adults capable of breathing air, and lay eggs in water. \\n\\nThis additional information is relevant and accurate, and it enhances the helpfulness of the submission by providing a more comprehensive answer to the user's question. \\n\\nTherefore, the submission meets the criterion of helpfulness.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('c6e2d019-701f-45c5-9707-120732ed7701'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None)],\n",
       "   'execution_time': 1.234969,\n",
       "   'run_id': '0628ff80-6c46-4c11-a127-020b800e0add',\n",
       "   'output': AIMessage(content='The main characteristic of amphibians is that they have a dual life cycle, meaning they typically live both on land and in water. They are cold-blooded animals with moist, smooth skin, and most species begin life as water-breathing larvae before metamorphosing into adults capable of breathing air. Amphibians also typically lay eggs in water, where the young develop before emerging onto land.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 78, 'prompt_tokens': 16, 'total_tokens': 94, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bp5GoYZLKlHVyFJw9r7iehSqXh2GB', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--0628ff80-6c46-4c11-a127-020b800e0add-0', usage_metadata={'input_tokens': 16, 'output_tokens': 78, 'total_tokens': 94, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "   'reference': {'answer': 'They live both in water and on land'}},\n",
       "  '8fb7b28f-b53d-45b4-90ef-664623e3953f': {'input': {'question': 'What are reptiles known for?'},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment='The criterion for this task is the helpfulness of the AI\\'s response, taking into account the correct reference answer. \\n\\nThe reference answer is \"Having scales\". \\n\\nThe AI\\'s response includes the information that reptiles are known for \"having scales or scutes\", which matches the reference answer. \\n\\nIn addition to this, the AI\\'s response provides further information about what reptiles are known for, such as being cold-blooded, laying eggs, shedding their skin, their diverse range of habitats, their unique adaptations for survival, and examples of common reptiles. \\n\\nThis additional information is relevant and could be considered helpful to the user, as it provides a more comprehensive answer to the question. \\n\\nTherefore, the AI\\'s response meets the criterion of being helpful to the user, taking into account the correct reference answer. \\n\\nY', correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('6b21d1f3-10b7-4170-abf2-f71ad759f675'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None)],\n",
       "   'execution_time': 1.299077,\n",
       "   'run_id': 'dda776b1-15dc-4d9f-a70e-29b56982e761',\n",
       "   'output': AIMessage(content='Reptiles are known for being cold-blooded, having scales or scutes, and laying eggs. They are also known for their ability to shed their skin, their diverse range of habitats, and their unique adaptations for survival. Some common reptiles include snakes, turtles, lizards, and crocodiles.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 63, 'prompt_tokens': 14, 'total_tokens': 77, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bp5GnWtfdCxWhrGSWlr0t3qITQlz7', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--dda776b1-15dc-4d9f-a70e-29b56982e761-0', usage_metadata={'input_tokens': 14, 'output_tokens': 63, 'total_tokens': 77, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "   'reference': {'answer': 'Having scales'}},\n",
       "  '9af62297-ca17-487f-b8de-e051a79672f1': {'input': {'question': 'What do mammals and birds have in common?'},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"The criterion for this task is the helpfulness of the AI's submission, taking into account the correct reference answer. \\n\\nThe reference answer states that mammals and birds are both warm-blooded. \\n\\nLooking at the AI's submission, it does mention that mammals and birds are both warm-blooded, which aligns with the reference answer. \\n\\nHowever, the AI's submission goes beyond the reference answer and provides additional information about the common characteristics of mammals and birds, such as having a backbone, hair or feathers, giving birth to live young, providing milk to their offspring, having lungs for respiration, a four-chambered heart for circulation, and a highly developed brain and nervous system. \\n\\nWhile this additional information is not part of the reference answer, it does not contradict the reference answer and could be considered helpful to the user by providing a more comprehensive answer to the question. \\n\\nTherefore, the AI's submission can be considered helpful and meets the criterion.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('d5218890-26e1-488c-abca-8d013ec29883'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None)],\n",
       "   'execution_time': 1.533023,\n",
       "   'run_id': '3a726774-2f54-4d9a-9e73-2f6fbcb25307',\n",
       "   'output': AIMessage(content='Mammals and birds are both warm-blooded vertebrate animals that have similar characteristics such as having a backbone, hair or feathers to regulate body temperature, giving birth to live young, and providing milk to their offspring. They both have lungs for respiration and a four-chambered heart for circulation. Additionally, they both possess a highly developed brain and nervous system.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 74, 'prompt_tokens': 16, 'total_tokens': 90, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bp5GoEqnhgCxC44wV38Qyr1BGz9aa', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--3a726774-2f54-4d9a-9e73-2f6fbcb25307-0', usage_metadata={'input_tokens': 16, 'output_tokens': 74, 'total_tokens': 90, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "   'reference': {'answer': 'They are both warm-blooded'}},\n",
       "  '60fc1caa-6411-4a84-89d9-4397b8196f08': {'input': {'question': 'What is the largest mammal?'},\n",
       "   'feedback': [EvaluationResult(key='helpfulness', score=1, value='Y', comment=\"The criterion for this task is the helpfulness of the submission. The submission is considered helpful if it provides the correct answer to the user's question.\\n\\nLooking at the input, the user asked for the largest mammal. The reference answer confirms that the correct answer to this question is the blue whale.\\n\\nThe AI's submission correctly identifies the blue whale as the largest mammal. In addition, it provides extra information about the size and weight of the blue whale, which could be considered as adding to the helpfulness of the response.\\n\\nTherefore, the submission meets the criterion of being helpful to the user.\\n\\nY\", correction=None, evaluator_info={'__run': RunInfo(run_id=UUID('50c2fc16-b74c-455f-8aa6-defd91226c23'))}, feedback_config=None, source_run_id=None, target_run_id=None, extra=None)],\n",
       "   'execution_time': 0.883397,\n",
       "   'run_id': '999d4617-43b8-479e-a07c-1e63d6da9427',\n",
       "   'output': AIMessage(content='The largest mammal is the blue whale (Balaenoptera musculus), which can grow up to 100 feet in length and weigh up to 200 tons.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 14, 'total_tokens': 49, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-Bp5GnzkNUxJbeg3Mn6Nm7F7FQG5GX', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--999d4617-43b8-479e-a07c-1e63d6da9427-0', usage_metadata={'input_tokens': 14, 'output_tokens': 35, 'total_tokens': 49, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "   'reference': {'answer': 'The blue whale'}}},\n",
       " 'aggregate_metrics': None}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Evaluate Datasets With Customized Criterias\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\n",
    "        RunEvalConfig.LabeledCriteria(\n",
    "            {\n",
    "                \"helpfulness\": (\n",
    "                    \"Is this submission helpful to the user,\"\n",
    "                    \" taking into account the correct reference answer?\"\n",
    "                )\n",
    "            }\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Elementary Animal Questions\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Evaluate Datasets Without Labels\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\n",
    "        RunEvalConfig.Criteria(\n",
    "            {\"creativity\": \"Is this submission creative, imaginative, or novel?\"}\n",
    "        ),\n",
    "        # We provide some simple default criteria like \"conciseness\" you can use as well\n",
    "        RunEvalConfig.Criteria(\"conciseness\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Rap Battle Dataset\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Evaluate Datasets Based on Cosine Distance Criteria\n",
    "# Cosine Distance: Ranged Between 0 to 1. 0 = More Similar\n",
    "\n",
    "evaluation_config = RunEvalConfig(\n",
    "    evaluators=[\n",
    "        # You can define an arbitrary criterion as a key: value pair in the criteria dict\n",
    "        \"embedding_distance\",\n",
    "        # Or to customize the embeddings:\n",
    "        # Requires 'pip install sentence_transformers'\n",
    "        # RunEvalConfig.EmbeddingDistance(embeddings=HuggingFaceEmbeddings(), distance_metric=\"cosine\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "run_on_dataset(\n",
    "    client=client,\n",
    "    dataset_name=\"Elementary Animal Questions\",\n",
    "    llm_or_chain_factory=llm,\n",
    "    evaluation=evaluation_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂΩìÂâçÊï∞ÊçÆÈõÜ: [Dataset(name='Elementary Animal Questions', description='Questions and answers about animal phylogenetics.', data_type=<DataType.kv: 'kv'>, id=UUID('d8682549-f339-411b-9772-261bdddff364'), created_at=datetime.datetime(2025, 6, 26, 5, 36, 38, 26943, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 6, 26, 5, 36, 38, 26943, tzinfo=datetime.timezone.utc), example_count=4, session_count=3, last_session_start_time=datetime.datetime(2025, 6, 26, 5, 44, 5, 953506), inputs_schema=None, outputs_schema=None, transformations=None), Dataset(name='Rap Battle Dataset', description='Rap battle prompts.', data_type=<DataType.kv: 'kv'>, id=UUID('2013dbcd-1ca9-4e41-b1a3-9990b2b4698c'), created_at=datetime.datetime(2025, 6, 26, 5, 13, 36, 904799, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 6, 26, 5, 13, 36, 904799, tzinfo=datetime.timezone.utc), example_count=4, session_count=5, last_session_start_time=datetime.datetime(2025, 6, 26, 5, 43, 43, 101001), inputs_schema=None, outputs_schema=None, transformations=None), Dataset(name='Example Dataset', description='An example dataset', data_type=<DataType.kv: 'kv'>, id=UUID('ee4c8564-b6e7-4e40-aaea-573b21f37e57'), created_at=datetime.datetime(2025, 6, 26, 5, 38, 48, 527973, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 6, 26, 5, 38, 48, 527973, tzinfo=datetime.timezone.utc), example_count=7, session_count=0, last_session_start_time=None, inputs_schema=None, outputs_schema=None, transformations=None), Dataset(name='My CSV Dataset', description='Dataset created from a CSV file', data_type=<DataType.kv: 'kv'>, id=UUID('416fc082-2f9c-498a-bbea-6c92b7cb6c37'), created_at=datetime.datetime(2025, 6, 26, 5, 41, 26, 291458, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 6, 26, 5, 41, 26, 291458, tzinfo=datetime.timezone.utc), example_count=4, session_count=0, last_session_start_time=None, inputs_schema=None, outputs_schema=None, transformations=None), Dataset(name='My Dataframe Dataset', description='Dataset created from a dataframe', data_type=<DataType.kv: 'kv'>, id=UUID('fa68099a-c20d-4b27-aa4d-894a05e3d7f2'), created_at=datetime.datetime(2025, 6, 26, 5, 40, 30, 86623, tzinfo=datetime.timezone.utc), modified_at=datetime.datetime(2025, 6, 26, 5, 40, 30, 86623, tzinfo=datetime.timezone.utc), example_count=4, session_count=0, last_session_start_time=None, inputs_schema=None, outputs_schema=None, transformations=None)] \n",
      "üóëÔ∏è Âà†Èô§Êï∞ÊçÆÈõÜ: Elementary Animal Questions (ID: d8682549-f339-411b-9772-261bdddff364)\n",
      "üóëÔ∏è Âà†Èô§Êï∞ÊçÆÈõÜ: Rap Battle Dataset (ID: 2013dbcd-1ca9-4e41-b1a3-9990b2b4698c)\n",
      "üóëÔ∏è Âà†Èô§Êï∞ÊçÆÈõÜ: Example Dataset (ID: ee4c8564-b6e7-4e40-aaea-573b21f37e57)\n",
      "üóëÔ∏è Âà†Èô§Êï∞ÊçÆÈõÜ: My CSV Dataset (ID: 416fc082-2f9c-498a-bbea-6c92b7cb6c37)\n",
      "üóëÔ∏è Âà†Èô§Êï∞ÊçÆÈõÜ: My Dataframe Dataset (ID: fa68099a-c20d-4b27-aa4d-894a05e3d7f2)\n"
     ]
    }
   ],
   "source": [
    "# Âà†Èô§Êï∞ÊçÆÈõÜ\n",
    "\n",
    "# Ëé∑ÂèñÊâÄÊúâÊï∞ÊçÆÈõÜ\n",
    "datasets = list(client.list_datasets())\n",
    "print(f\"ÂΩìÂâçÊï∞ÊçÆÈõÜ: {datasets} \")\n",
    "\n",
    "# ‚úÖ ÊâπÈáèÂà†Èô§\n",
    "for ds in datasets:\n",
    "    print(f\"üóëÔ∏è Âà†Èô§Êï∞ÊçÆÈõÜ: {ds.name} (ID: {ds.id})\")\n",
    "    client.delete_dataset(dataset_id = ds.id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
